{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    " Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is 0\n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|                                  | **Unsatisfactory**                                                                                                                                                                                                                                                                                                                        | **Developing**                                                                                                                                                                                                       | **Proficient**                                                                                                                                                                                            | **Excellent**                                                                                                                                                                            |\n",
    "|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **EDA relevance**                | EDA is mostly neither relevant to the question nor helpful in figuring out how to address the question. Or the EDA does address the question, but many obviously relevant variables / analyses / figures were not included. | EDA is partly irrelevant/unhelpful. EDA missed one or two obvioulsy relevant analysis (distributions of single variables or relationships between variables) | EDA includes the obviously relevant / helpful variables in addressing the question.                                                              | Thorough EDA fully explored the dataset                                                                                                                 |\n",
    "| **EDA analysis and description** | Many of the analyses are poor choices (e.g., using means instead of medians for obviously skewed data), or are poorly described in the text, or do not aid understanding the data                                                                                                                                                     | Some of the analyses are poor choices, or are poorly described in the text, or do not aid understanding the data                                                                                                 | All analyses are correct choices. Only one or two have minor issues in the text descriptions supporting them. Mostly they fit well with other elements of the EDA and support understanding the data  | All analyses are correct choices with clear text descriptions supporting them. The figures fit well with the other elements of the EDA, producing a clear understanding of the data. |\n",
    "| **EDA figures**                  | Many of the figures are poor plot choices (e.g., using a bar plot to represent a time series where it would be better to use a line plot) or have poor aesthetics (including colormap, data point shape/color, axis labels, titles, annotations, text legibility) or do not aid understanding the data                                | Some of the figures are poor plot choices or have poor aesthetics. Some figures do not aid understanding the data                                                                                                | All figures are correct plot choices. Only one or two have minor questionable aesthetic choices. The figures mostly fit well with the other elements of the EDA and support understanding the data    | All figures are correct plot choices with beautiful aesthetics. The figures fit well with the other elements of the EDA, producing a clear understanding of the data.                |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - EDA Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "- Eric Covington: Data curation, Project administration, Writing - original draft\n",
    "- Claire Park: Analysis, Background research, Writing - original draft\n",
    "- Ethan Han: Data tidying and cleaning, Data curation, Writing - original draft\n",
    "- Qinyi Chen: Analysis, Background research, Writing - original draft\n",
    "- Ziche Liu: Analysis, Data curation, Background research, Writing - original draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General: How do animal traits, intake conditions, and neighborhood context jointly predict length-of-stay (LOS) for cats and dogs at Austin Animal Center, and which factors offer the biggest, actionable reductions in LOS?\n",
    "\n",
    "- **Sub-Q1 (animal traits, predictive):** To what extent do age, species, sterilization status, sex, and breed type predict LOS, and which have the highest feature importance in a supervised model?\n",
    "- **Sub-Q2 (intake conditions, causal/associational):** Controlling for traits, how do intake type (stray, surrender, transfer) and intake health condition shift expected LOS?\n",
    "- **Sub-Q3 (neighborhood context):** After geocoding found-location, do animals from census tracts with higher poverty/unstable housing show longer LOS, holding traits and intake conditions constant?\n",
    "- **Sub-Q4 (time/operations):** Are there seasonal or capacity-related patterns (month/weekday, intake volume) that systematically lengthen LOS?\n",
    "- **Sub-Q5 (actionability):** Which predictors remain most influential in a parsimonious model, indicating levers (e.g., fast-track for healthy juveniles, targeted outreach for high-LOS tracts) to shorten LOS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With millions of animals continuing to enter shelters, limited housing capacity, staffing, and funding in many communities have forced shelters to triage scarce resources，determining which animals can be made available for adoption and, in the worst cases, which animals cannot be maintained safely in care. **Overall, from 2013–2025, the U.S. sheltering landscape shows a mixed trajectory: measurable lifesaving gains in the mid-2010s followed by renewed system strain after 2020, making shelter “throughput” (e.g., length-of-stay, LOS) increasingly central to sustaining high live outcomes.**\n",
    "\n",
    "\n",
    "**From 2013 to 2020, national estimates suggest progress, though uneven and still capacity-limited.** The ASPCA reported euthanasia declining to about 1.5 million annually (down from about2.6 million in 2011) alongside an 18.5% increase in adoptions to roughly 3.2 million per year [1]. Best Friends’ 2020 national summary similarly framed remaining need as a lifesaving gap and reported improved national performance (e.g., an 83% save rate and growth in the share of “no-kill” shelters), while emphasizing that progress varied across communities [2].\n",
    "\n",
    "**After 2020, capacity stress became more visible in national reporting, strengthening the case for LOS-focused analysis.** Shelter Animals Count described a sustained “capacity crisis” since January 2021 and reported that 2023 dog euthanasia reached 359,000 (with 2023 also the first year in their database when more dogs than cats were euthanized) [3][4]. The ASPCA reports 4.2 million adoptions and about 607,000 euthanasias in 2024, noting adoptions were not enough to significantly reduce the number of animals in shelters nationwide [5]. **Prior work indicates LOS depends on both animal traits and shelter operations and can be context-specific:** Brown et al. (2013) found LOS varied with factors such as age and size in two no-kill shelters [6], while some appearance variables were not consistently predictive, and Capacity for Care evaluations suggest management practices can shift adoption-related timing and outcomes [7]. Austin is an especially strong case because the city has explicitly prioritized no-kill goals through public planning and partnerships [8][9], reports very high live release performance (e.g., 97.5% in a November 2015 City report) [10], and provides open Intakes and Outcomes data from Oct 1, 2013 to May 5, 2025 that enable direct LOS computation at scale [11][12].\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[1] ASPCA. ASPCA releases new data showing remarkable progress for homeless dogs and cats. https://www.aspca.org/about-us/press-releases/aspca-releases-new-data-showing-remarkable-progress-homeless-dogs-cats\n",
    "\n",
    "[2] Best Friends Animal Society. The State of U.S. Animal Sheltering, 2020. https://bestfriends.org/network/studies-publications/state-us-animal-sheltering-2020\n",
    "\n",
    "[3] Shelter Animals Count. SAC releases 2023 annual analysis. https://www.shelteranimalscount.org/sac-releases-2023-annual-analysis/\n",
    "\n",
    "[4] Shelter Animals Count. How many dogs are euthanized in the U.S. every year? In 2023 the number surpassed cats. https://www.shelteranimalscount.org/how-many-dogs-are-euthanized-in-the-us-every-year-in-2023-the-number-surpassed-cats/\n",
    "\n",
    "[5] ASPCA. U.S. animal shelter statistics (2024). https://www.aspca.org/helping-shelters-people-pets/us-animal-shelter-statistics\n",
    "\n",
    "[6] Brown WP, Davidson JP, Zuefle ME. Effects of phenotypic characteristics on the length of stay of dogs at two no-kill animal shelters. https://pubmed.ncbi.nlm.nih.gov/23282290/\n",
    "\n",
    "[7] Karsten CL, Wagner DC, Kass PH, Hurley KF. An observational study of the relationship between Capacity for Care… https://www.sciencedirect.com/science/article/pii/S1090023317301545\n",
    "\n",
    "[8] City of Austin. No-Kill Plan. https://www.austintexas.gov/page/no-kill-plan\n",
    "\n",
    "[9] Austin Pets Alive! Our Story. https://www.austinpetsalive.org/about/our-story\n",
    "\n",
    "[10] City of Austin. Animal Services Report (Nov 2015). https://www.austintexas.gov/edims/document.cfm?id=243934\n",
    "\n",
    "[11] City of Austin Open Data. Austin Animal Center Intakes (Oct 1, 2013–May 5, 2025). https://catalog.data.gov/dataset/austin-animal-center-intakes\n",
    "\n",
    "[12] City of Austin Open Data. Austin Animal Center Outcomes (Oct 1, 2013–May 5, 2025). https://catalog.data.gov/dataset/austin-animal-center-outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect LOS to be shortest for younger, already-sterilized animals with healthy intake condition and standard stray/owner-surrender intake types, even after adjusting for species and breed type. Animals originating from tracts with higher poverty or unstable housing will have longer LOS, independent of individual traits. Seasonal and high-capacity periods will further lengthen LOS, but trait and intake-condition effects will remain the primary drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "- #### Dataset #1\n",
    "  - Dataset Name: Austin Animal Center Intakes (2013-10-01 to 2025-05-05)\n",
    "  - Link to the dataset: https://catalog.data.gov/dataset/austin-animal-center-intakes\n",
    "  - Number of observations: 173,812\n",
    "  - Number of variables: 12 columns\n",
    "  - Key fields: Animal ID, intake DateTime, intake type (e.g., stray, owner surrender), intake condition, animal type, breed, color, sex upon intake, age upon intake.\n",
    "  - Shortcomings: Single-shelter scope; breed/age/sex often estimated; missing names and some sex/age values marked \"unknown\"; repeated animals may appear as multiple events.\n",
    "\n",
    "- #### Dataset #2\n",
    "  - Dataset: Austin Animal Center Outcomes (2013-10-01 to 2025-05-05)\n",
    "  - Link: https://catalog.data.gov/dataset/austin-animal-center-outcomes\n",
    "  - Observations: 173,775 outcome events.\n",
    "  - Variables: 12 columns.\n",
    "  - Key fields: Animal ID, outcome DateTime, outcome type (adoption, transfer, return to owner, euthanasia, etc.), outcome subtype, animal type, sex upon outcome, age upon outcome, breed, color.\n",
    "  - Shortcomings: Outcome subtype missing for many records (~54%); single no-kill shelter so results may not generalize; breed/age/sex can be estimated; duplicate IDs exist and need removal.\n",
    "\n",
    "- **Combining plan**: Join intakes and outcomes on Animal ID (and aligned date ranges) to compute length of stay and link intake attributes to outcome results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://drive.google.com/uc?export=download&id=15AIylrbBsPzUqwFoXkDAzISE6gZrFNGt', 'filename':'intakes.csv'},\n",
    "    { 'url': 'https://drive.google.com/uc?export=download&id=1owphJpwvSuutUaYZxmZ6uNuhd18HPUcy', 'filename':'outcomes.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austin Animal Center Intakes (10/01/2013 to 05/05/2025) \n",
    "\n",
    "The dataset we used comes from the Austin Animal Center and contains records of animals that entered the shelter system in Austin, Texas between October 2013 and May 2021. It includes information about individual animals at the time they were brought into the shelter, such as intake date and time, intake type, animal type, breed, color, sex upon intake, age upon intake, and intake condition. Each row represents a single intake event for an animal, meaning the dataset is structured at the event level rather than the individual animal level.\n",
    "\n",
    "Several important metrics are included in this dataset. The DateTime variable records the exact timestamp of intake in standard date-time format, which allows analysis of seasonal or temporal patterns in shelter intake volume. Age upon intake is typically reported in units such as years, months, weeks, or days, representing the estimated age of the animal at intake; this variable may require conversion into a consistent numerical unit (for example, years) for statistical analysis. Animal Type indicates species (e.g., dog, cat, or other animals), while Breed and Color provide descriptive categorical attributes that may influence intake trends or outcomes. Intake Type (e.g., stray, owner surrender, public assist) describes how the animal entered the shelter system and is an important variable for understanding shelter operations and community behavior. Intake Condition describes the health or physical status of the animal upon arrival and may relate to medical resource needs or survival outcomes.\n",
    "\n",
    "There are several potential concerns with this dataset. First, the data only represent animals that were brought to the Austin Animal Center and therefore do not reflect all animals in the Austin area. Animals that were never captured, taken to private shelters, or kept by finders are not included, which introduces geographic and institutional sampling bias. Second, some variables rely on human estimation or reporting, such as breed identification and age estimation, which may introduce measurement error or inconsistency. Shelter staff may visually estimate breed, which is known to be imperfect, especially for mixed-breed animals.\n",
    "\n",
    "Additionally, missing data may occur in variables such as sex upon intake or age upon intake, particularly for stray animals where information is unknown. There may also be inconsistencies in categorical labels over time due to changes in data entry practices or shelter procedures. Finally, repeated intake events for the same animal could appear as multiple records if animals re-enter the shelter, which could affect analyses if not properly accounted for depending on the research question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = pd.read_csv('data/00-raw/intakes.csv')\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 173812 entries in the dataset.\n",
    "\n",
    "All datatypes in the dataset are objects. All attributes except Name have no non-null values. Sex and age upon intake has missing values that are labelled as Unknown. Age is represented by a string. We will calculate the approximate age by days, and categorize into buckets. Column names must be renamed for ease of use. There are 36 duplicates that must be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to change the names of the columns to be more workable. Lets also do preliminary cleaning by snake-casing all of the values. We also need to convert the string DateTime to an actual DateTime and get rid of the redundant MonthYear column. Finally, we should split up the Sex upon Intake variable. It contains information for both gender and spay status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert names into usable strings\n",
    "df_intakes.columns = df_intakes.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Tidying dates\n",
    "df_intakes['datetime'] = pd.to_datetime(df_intakes['datetime'])\n",
    "df_intakes['year'] = df_intakes['datetime'].dt.year\n",
    "df_intakes['month'] = df_intakes['datetime'].dt.month\n",
    "df_intakes = df_intakes.drop('monthyear', axis=1)\n",
    "\n",
    "# Tidying values\n",
    "string_cols = df_intakes.select_dtypes(include=['object']).columns\n",
    "for col in string_cols:\n",
    "    df_intakes[col] = df_intakes[col].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# Tidying sex\n",
    "sex_split = df_intakes['sex_upon_intake'].str.split(' ', expand=True)\n",
    "df_intakes['sex'] = sex_split[1].fillna(sex_split[0])\n",
    "df_intakes['spay_neuter'] = sex_split[0]\n",
    "df_intakes['sex'] = df_intakes['sex'].replace('unknown', 'unknown')\n",
    "df_intakes['spay_neuter'] = df_intakes['spay_neuter'].replace('unknown', 'unknown')\n",
    "df_intakes = df_intakes.drop(['sex_upon_intake'], axis=1)\n",
    "\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also tidy up the multi-value color and breed attribute by giving each value its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_split = df_intakes['color'].str.split('/', expand=True)\n",
    "df_intakes['primary_color'] = color_split[0]\n",
    "df_intakes['secondary_color'] = color_split[1].fillna('solid')\n",
    "\n",
    "breed_split = df_intakes['breed'].str.split('/', expand=True)\n",
    "df_intakes['primary_breed'] = breed_split[0]\n",
    "df_intakes['secondary_breed'] = breed_split[1].fillna('none')\n",
    "\n",
    "df_intakes = df_intakes.drop(['color','breed'], axis=1)\n",
    "# get rid of 'mix'\n",
    "df_intakes['primary_breed'] = df_intakes['primary_breed'].str.replace(' mix', '', regex=False)\n",
    "\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace the missing names with 'Unknown'. We shouldn't remove it because a significant portion of stray animals have no known name. Then, we should convert the names into a categorical variable that represents if an animal has a name or not. The actual name will most likely not be useful. We will assign it as follows: 1=named, 0=unnamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['name'] = df_intakes['name'].fillna('Unknown')\n",
    "df_intakes['has_name'] = (df_intakes['name'] != 'Unknown').astype(int)\n",
    "df_intakes = df_intakes.drop(['name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will handle the awkward age feature. This feature is a string that represents an estimation of the age of the animal. We should convert this into an ordinal variable that represents the animal's age in months. We should do this because the types of animals in the shelter vary dramatically in rate of aging. In addition, this will make using the data more efficient because we won't have to one hot encode a bunch of age categories. \n",
    "\n",
    "First, lets define our age to months method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_to_months(age_str):\n",
    "    if pd.isna(age_str) or age_str == 'Unknown':\n",
    "        return np.nan\n",
    "    \n",
    "    parts = str(age_str).split()\n",
    "    if len(parts) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    number = int(parts[0])\n",
    "    unit = parts[1].lower()\n",
    "    \n",
    "    if 'year' in unit:\n",
    "        return number * 12\n",
    "    elif 'month' in unit:\n",
    "        return number\n",
    "    elif 'week' in unit:\n",
    "        return number / 4.34  # average weeks in a month\n",
    "    elif 'day' in unit:\n",
    "        return number / 30.44 # aaverage days in a month\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create the new attribute. We will fill in the age value with the median age. We will fill with the median because it is resistant to outliers, which will be a problem due to the wide variety of animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['age_months'] = df_intakes['age_upon_intake'].apply(age_to_months)\n",
    "\n",
    "df_intakes['age_months'] = df_intakes['age_months'].round(1)\n",
    "\n",
    "median_age = df_intakes['age_months'].median()\n",
    "df_intakes['age_months'] = df_intakes['age_months'].fillna(median_age)\n",
    "\n",
    "df_intakes = df_intakes.drop(['age_upon_intake'], axis=1)\n",
    "\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets inspect the data for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- AGE DISTRIBUTION ---\")\n",
    "print(df_intakes['age_months'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- INTAKE CONDITIONS ---\")\n",
    "print(df_intakes['intake_condition'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some negative ages. These columns have incorrect data and must be dropped. One point to notice is that there are also huge spikes at 1 and 2 years in months. This means the staff at the shelters are likely estimating many of the ages of the animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes[df_intakes['age_months'] >= 0]\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. Now lets group medical intake conditions as 'medical' and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_map = {\n",
    "    # Medical issues\n",
    "    'sick': 'medical',\n",
    "    'injured': 'medical',\n",
    "    'med attn': 'medical',\n",
    "    'med urgent': 'medical',\n",
    "    'medical': 'medical',\n",
    "    'neurologic': 'medical',\n",
    "    'parvo': 'medical',\n",
    "    'panleuk': 'medical',\n",
    "    'congenital': 'medical',\n",
    "    \n",
    "    # Life stage / Reproduction\n",
    "    'nursing': 'vulnerable',\n",
    "    'neonatal': 'vulnerable',\n",
    "    'pregnant': 'vulnerable',\n",
    "    \n",
    "    # Age/Critical\n",
    "    'aged': 'critical_senior',\n",
    "    'agonal': 'critical_senior',\n",
    "    \n",
    "    # Behavioral\n",
    "    'feral': 'behavioral',\n",
    "    'behavior': 'behavioral',\n",
    "    \n",
    "    'normal': 'normal'\n",
    "}\n",
    "\n",
    "df_intakes['condition_simple'] = df_intakes['intake_condition'].map(intake_map).fillna('other_unknown')\n",
    "df_intakes['condition_simple'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are joining this dataset with the outcomes dataset, we must specify which datetime corresponds to intake and outcome. Therefore, we must rename the datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.rename(columns={'datetime': 'intake_datetime'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also drop the found location attribute since we will not be using it for this checkpoint. We plan to add geocoding of the location address and cross reference it with spatial poverty and crime data at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes = df_intakes.drop(['found_location'], axis=1)\n",
    "df_intakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "The data is now cleaned and tidied. The composite attributes have been split up into separate columns. Irrelevant columns have been dropped and nulls have been filled. The approximate age attribute has been converted into an ordinal feature. We have filled the missing ages with the median because it will be safe against the wide variety of values due to the variety of animals in the shelters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes.to_csv('data/01-interim/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austin Animal Center Outcomes (10/01/2013 to 05/05/2025)\n",
    "\n",
    "This dataset also comes from the Austin Animal shelter, containing the records of animals that left the shelter system in Austin, Texas during the same time period as the previous dataset. The information it includes about these animals are its date of birth, name, the date and time the animal left the shelter, the outcome type and subtype (e.g. where the animal went to after it left), sex upon outcome, age upon outcome, breed, and color. Like the previous dataset, this dataset is structured at the event level over the individual animal level. \n",
    "\n",
    "Out of the variables used for both datasets, the animal ID, animal type, breed, color, and birthdate (if listed) can be used to join or cross-reference the two datasets.\n",
    "\n",
    "Of the important metrics in the dataset, the categories Outcome Type and Outcome Subtype are not included nor is a variant of another metric in the previous dataset and so needs an explanation. Outcome Type lists what happened to the animal as they left the shelter, with values such as transfer, adoption, return to owner, and euthanization. Outcome Subtype contains the location they ended up or the reason for their departure, providing additional context as to why the animal left. These are important metrics because as a no-kill shelter (having a non-euthanization rate of at least 90%), they provide information about where the animal goes if it is sick or the shelter gets overpopulated.\n",
    "\n",
    "Several potential concerns with this dataset concerns its ability to generalize to all animal shelter outcomes. As mentioned above, this data is from a no-kill shelter, so the shelter will do all it can to not euthanize animals. This will lead to drastically different outcomes than an open admissions shelter, which do euthanize animals when the shelter becomes overpopulated. Another concern is that since this dataset is taken from only one shelter in a specific city, it does not reflect all outcomes in the entire city— let alone the state or the country. In fact, Texas is one of the biggest exporters of shelter animals, shipping them to areas with high demand for pet animals such as New York. This means that the data from this shelter may be significantly different from any data collected in other animal shelters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outcomes = pd.read_csv('data/00-raw/outcomes.csv')\n",
    "df_outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outcomes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 173775 entries in this dataset.\n",
    "\n",
    "There are only null values in outcome type. Since this is vital information for our hypothesis, we must get rid of the rows that do not have this feature in both intakes and outcomes. \n",
    "Since the relevant information in this dataset is limited to animal ID, outcome type, outcome subtype and LOS, we can discard all other columns. We can compute LOS by subtracting the difference between the intake datetime and the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidying the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, lets make the column titles and data lowercase for uniformity. Lets also convert the date string into a datetime. Lets also quickly drop the irrelevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert names into usable strings\n",
    "df_outcomes.columns = df_outcomes.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Tidying dates\n",
    "df_outcomes['datetime'] = pd.to_datetime(df_outcomes['datetime'], format='ISO8601', errors='coerce')\n",
    "df_outcomes['year'] = df_outcomes['datetime'].dt.year\n",
    "df_outcomes['month'] = df_outcomes['datetime'].dt.month\n",
    "df_outcomes = df_outcomes.rename(columns={'datetime': 'outcome_datetime'})\n",
    "\n",
    "# Tidying values\n",
    "string_cols = df_outcomes.select_dtypes(include=['object']).columns\n",
    "for col in string_cols:\n",
    "    df_outcomes[col] = df_outcomes[col].str.lower().str.strip()\n",
    "\n",
    "df_outcomes = df_outcomes.drop(['name', 'date_of_birth', 'monthyear', 'animal_type','sex_upon_outcome', 'age_upon_outcome', 'breed','color','year','month'], axis=1)\n",
    "df_outcomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data\n",
    "Now we need to remove unusuable rows and duplicates. The outcome type is crucial information, so any outcomes without an outcome will be removed. If an animal has an outcome type but no subtype, we will replace NaN with the string 'none' to be used as a separate category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_outcomes = df_outcomes.dropna(subset=['outcome_type'])\n",
    "df_outcomes['outcome_subtype'] = df_outcomes['outcome_subtype'].fillna('none')\n",
    "\n",
    "df_outcomes = df_outcomes.drop_duplicates()\n",
    "\n",
    "df_intakes_interim = pd.read_csv('data/01-interim/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the dataframes\n",
    "We must be careful when we join the intakes and outcomes. If an animal gets admitted at time A and at time B and also has outcomes for time A and B, doing a natural join (pd.merge) on animal_id will cause spurious tuples. The correct way is to merge them chronologically so we retain the correct order of intakes and outcomes for each animal. To do this we will sort by datetime and then merge in forward order.\n",
    "\n",
    "In order to sort, we must localize the timezones, since the format differs between the datasets. These two datasets have a timezone mismatch, so first we will remove the timezones. We can do this because all intakes and outcomes took place in Austin, TX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intakes['intake_datetime'] = pd.to_datetime(df_intakes['intake_datetime']).dt.tz_localize(None)\n",
    "\n",
    "df_outcomes['outcome_datetime'] = pd.to_datetime(df_outcomes['outcome_datetime']).dt.tz_localize(None)\n",
    "\n",
    "df_intakes = df_intakes.sort_values('intake_datetime')\n",
    "df_outcomes = df_outcomes.sort_values('outcome_datetime')\n",
    "\n",
    "df_combined = pd.merge_asof(\n",
    "    df_intakes, \n",
    "    df_outcomes, \n",
    "    left_on='intake_datetime', \n",
    "    right_on='outcome_datetime', \n",
    "    by='animal_id', \n",
    "    direction='forward'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to complete our dataset, we must create the new feature 'days_in_shelter', which is the time difference between intake and outcome datetime. We will calculate the difference in days and store the result in a float called 'days_in_shelter'. We will round to the nearest two decimal places for neatness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['outcome_datetime'] = pd.to_datetime(df_combined['outcome_datetime']).dt.tz_localize(None)\n",
    "df_combined['intake_datetime'] = pd.to_datetime(df_combined['intake_datetime']).dt.tz_localize(None)\n",
    "\n",
    "df_combined['time_in_shelter'] = df_combined['outcome_datetime'] - df_combined['intake_datetime']\n",
    "df_combined['days_in_shelter'] = df_combined['time_in_shelter'].dt.total_seconds() / (24 * 3600)\n",
    "df_combined['days_in_shelter'] = df_combined['days_in_shelter'].round(2)\n",
    "df_combined = df_combined.drop('time_in_shelter', axis=1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Now our code is cleaned and ready to be used. We have removed irrelevant columns, removed rows with crucial missing data, and filled in the rest with a 'none' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('data/02-processed/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Exploratory Data Analysis\n",
    "\n",
    "Instructions: replace the words in this subsection with whatever words you need to setup and preview the EDA you're going to do.   \n",
    "\n",
    "Please explicitly load the fully wrangled data you will use from `data/02-processed`.  This is a good idea rather than forcing people to re-run the data getting / wrangling cells above.  Sometimes it takes a long time to get / wrangle data compared to reloading the fixed up dataset.\n",
    "\n",
    "Carry out whatever EDA you need to for your project in the code cells below.  Because every project will be different we can't really give you much of a template at this point. But please make sure you describe the what and why in text here as well as providing interpretation of results and context.\n",
    "\n",
    "Please note that you should consider the use of python modules in your work.  Any code which gets called repeatedly should be modularized. So if you run the same pre-processing, analysis or visualiazation on different subsets of the data, then you should turn that into a function or class.  Put that function or class in a .py file that lives in `modules/`.  Import the module you made and use it to get your work done.  For reference see `get_raw()` which is inside `modules/get_data.py`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Q1 (animal traits, predictive) of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Q2 (intake conditions, causal/associational) of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Q3 (neighborhood context) of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Q4 (time/operations) of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Q5 (actionability) of EDA - please give it a better title than this\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Data Collection\n",
    " - [ ] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "> Since our dataset is localized to a specific city (Austin, TX), the values we get from our data may differ significantly than in other animal shelters elsewhere, as pet adoption is not the same between states or even cities. Thus, our analysis may yield results that are not consistent across different regions. \n",
    "\n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    "> Our data is collected from a no-kill shelter, which means that at least 90% of the shelter's inhabitants are not euthanized. This may create  \n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    "> Our data is collected from a no-kill shelter, which means that at least 90% of the shelter's inhabitants are not euthanized. This will create discrepancies between the data of no-kill shelters such as this one and open-admission shelters. No-kill shelters may have higher rates of disability or illness amongst the animals, while open-admission shelters have higher rates of euthanization. \n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    "> Our results may affect what people look for in a pet and thus change the adoption outcomes of animals with certain traits or qualities. Some of these variables are related to conditions that some may find discriminatory, such as any potential disabilities that an animal might have. This would have a direct negative impact on the animals’ wellbeing and quality of life as a consequence of potential bias in our analysis. However, we believe that by showing the statistics of animals that are less often adopted, people would be more willing to adopt the less-adopted animals.\n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Team members will communicate using Discord. Responses are expected within a day.*\n",
    "* *Team members should be prepared to meet virtually or in-person at least once a week. Meetings will be scheduled on demand according to the project timeline. Attendance is mandatory.*\n",
    "* *Decisions will be made on a majority basis.*\n",
    "* *Deadlines will be set and tasks will be assigned during each team meeting.*\n",
    "* *When members are struggling to deliver tasks that have been assigned to them, it is their responsibility to contact the group at least one day before the deadline for help or reassignment. If it is the day of a deadline and a member has not reached out, another team member may assign themselves to the task.*\n",
    "* *Team Expectations will be posted in the group chat for all members to see.*\n",
    "* *Team members are expected to be civil and respectful in all communication.*\n",
    "* *All opinions will be heard and respected by the group.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/29  |  6:30 PM | Read & think about previous COGS 108 final projects  | Review and fill out project review questionnaire | \n",
    "| 2/1  |  10 AM |  Consider topic ideas and find interesting datasets/API available | Discuss ideal dataset(s); assign project proposal sections to group members |\n",
    "| ***2/4*** | ***Before 11:59 PM*** | ***Finish Final Project Proposal*** | ***Turn in Final Project Proposal*** |\n",
    "| 2/5 | 7 PM | Read through the Data Checkpoint notebook requirements | Go over the Data Checkpoint tasks and assign group members; remaining time for group/project work |\n",
    "| 2/12  | 7 PM  | Search for any last minute additional datasets | Data Check-in and triage issues/bottlenecks; remaining time for group/project work |\n",
    "| ***2/18*** | ***Before 11:59 PM*** | ***Finish remaining Data Checkpoint tasks*** | ***Turn in Final Project Data Checkpoint*** |\n",
    "| 2/19  | 7 PM  | Read through EDA Checkpoint notebook requirements | Go over the EDA Checkpoint tasks and assign group members; remaining time for group/project work |\n",
    "| 2/26  | 7 PM  | Data wrangling/EDA; Begin Analysis | EDA Check-in and triage issues/bottlenecks; remaining time for group/project work |\n",
    "| 3/5  | 7 PM  | Complete analysis; Draft results/conclusion/discussion | EDA Check-in and triage issues/bottlenecks; remaining time for group/project work |\n",
    "| ***3/9*** | ***Before 11:59 PM*** | ***Finish remaining EDA Checkpoint tasks*** | ***Turn in Final Project EDA Checkpoint*** |\n",
    "| 3/12 | 7 PM | Review Final Project turn-in requirements | Go over Final Project turn-in and assign any remaining tasks; remaining time for group/project work |\n",
    "| ***3/18***  | ***Before 11:59 PM***  | ***Finish remaining Final Project tasks*** | ***Turn in Final Project & Group Project Surveys*** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:COGS108_FA25]",
   "language": "python",
   "name": "conda-env-COGS108_FA25-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
